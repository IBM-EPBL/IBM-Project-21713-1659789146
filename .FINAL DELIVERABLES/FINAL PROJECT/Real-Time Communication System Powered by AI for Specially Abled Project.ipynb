{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c918afa5",
   "metadata": {},
   "source": [
    "**Real-Time Communication System Powered by AI for Specially Abled Project**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db4a8f1",
   "metadata": {},
   "source": [
    "\n",
    "**Image Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eb46b7",
   "metadata": {},
   "source": [
    "**Import ImageDataGenerator Library And Configure It**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df8b76f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ef382df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen=ImageDataGenerator(rescale=1./255,horizontal_flip=True,vertical_flip=True,zoom_range=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cbf210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ed0063",
   "metadata": {},
   "source": [
    "**Apply ImageDataGenerator Functionality To Train And Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9ea4555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15750 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "x_train=train_datagen.flow_from_directory(r\"C:\\Users\\Acer\\Downloads\\conversation engine for deaf and dumb\\Dataset\\training_set\",\n",
    "                                          target_size=(64,64),\n",
    "                                          class_mode=\"categorical\",batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d8c87f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2250 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "x_test=test_datagen.flow_from_directory(r\"C:\\Users\\Acer\\Downloads\\conversation engine for deaf and dumb\\Dataset\\test_set\",\n",
    "                                        target_size=(64,64),\n",
    "                                                            class_mode=\"categorical\",batch_size=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e0aaf0",
   "metadata": {},
   "source": [
    "**Model Building**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9089d9d9",
   "metadata": {},
   "source": [
    "**Import The Required Model Building Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9477ad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bc32b5",
   "metadata": {},
   "source": [
    "**Initialize The Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f0204d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89394e00",
   "metadata": {},
   "source": [
    "**Add The Convolution Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7cd435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Convolution2D(32,(3,3),activation=\"relu\",input_shape=(64,64,3)))\n",
    "#No of feature detectors, size of feature detector, image size, activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95305d4e",
   "metadata": {},
   "source": [
    "**Add The Pooling Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63ee8d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785169ae",
   "metadata": {},
   "source": [
    "**Add The Flatten Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60ad486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ede3c43",
   "metadata": {},
   "source": [
    "**Adding The Dense Layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7e4f2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(200,activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8094f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(200,activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0ffc163",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(9,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1355aeb",
   "metadata": {},
   "source": [
    "**Compile The Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "860295ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",metrics=[\"accuracy\"],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1de30fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "525"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94fd8734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fbb580",
   "metadata": {},
   "source": [
    "**Fit And Save The Model**\n",
    "Fit the neural network model with the train and test set, number of epochs, and validation steps.\n",
    "The weights are to be saved for future use. The weights are saved in signlanguage.h5 file using save()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1fd4ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      "525/525 [==============================] - 329s 616ms/step - loss: 0.3160 - accuracy: 0.8886 - val_loss: 0.1389 - val_accuracy: 0.9644\n",
      "Epoch 2/9\n",
      "525/525 [==============================] - 251s 478ms/step - loss: 0.0592 - accuracy: 0.9810 - val_loss: 0.2418 - val_accuracy: 0.9662\n",
      "Epoch 3/9\n",
      "525/525 [==============================] - 271s 515ms/step - loss: 0.0345 - accuracy: 0.9886 - val_loss: 0.2308 - val_accuracy: 0.9680\n",
      "Epoch 4/9\n",
      "525/525 [==============================] - 240s 457ms/step - loss: 0.0244 - accuracy: 0.9923 - val_loss: 0.1640 - val_accuracy: 0.9711\n",
      "Epoch 5/9\n",
      "525/525 [==============================] - 217s 412ms/step - loss: 0.0258 - accuracy: 0.9914 - val_loss: 0.0888 - val_accuracy: 0.9769\n",
      "Epoch 6/9\n",
      "525/525 [==============================] - 267s 509ms/step - loss: 0.0171 - accuracy: 0.9942 - val_loss: 0.2250 - val_accuracy: 0.9782\n",
      "Epoch 7/9\n",
      "525/525 [==============================] - 344s 655ms/step - loss: 0.0139 - accuracy: 0.9955 - val_loss: 0.1629 - val_accuracy: 0.9773\n",
      "Epoch 8/9\n",
      "525/525 [==============================] - 356s 678ms/step - loss: 0.0107 - accuracy: 0.9964 - val_loss: 0.1430 - val_accuracy: 0.9631\n",
      "Epoch 9/9\n",
      "525/525 [==============================] - 363s 692ms/step - loss: 0.0136 - accuracy: 0.9956 - val_loss: 0.2175 - val_accuracy: 0.9756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x231bd228e20>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,epochs=9,validation_data=x_test,steps_per_epoch=len(x_train),validation_steps=len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bda941b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"signlanguage-new.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9455acf7",
   "metadata": {},
   "source": [
    "**Test The Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9571c38d",
   "metadata": {},
   "source": [
    "**Import The Packages And Load The Saved Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c4b1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a71de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bebcca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model(\"signlanguage.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8cc105",
   "metadata": {},
   "source": [
    "**Load The Test Image, Pre-Process It And Predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bee53cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAAB0klEQVR4nO2Zv66CMBTG6Y0JsLixMJCYGOAJjAO7jGzwKjyCr+HCE7A78gAmDo6OPoA49Q7NJUQNl3Noy9H0NxHa0/Sj/U7/YFkGg8FgMBgMBgNF9vs9/2O9Xs/dnff8DJQtl8vu+XK53O939f0BMyTgCcdxHMdR1xUcbLiYc/4cwP4J0QxgBARRFHXPcRzHcdwvreu6s81isRAvkyQRb54q64C/8Hg8LMvK87z/crvdHg6H18pvWa1Wvu+LqBkESCdN088WwDkvigLdQ7CJFYHODUMm3u12uEYRoP09JCDLMlyjCG63Gy4QnEYV4XkeLpCKADRUBJzPZ1wgFQFoqKRRASKZ0hoB27ahIbQEfPwIIDACpBKGITSEVhay4DagNQIIyAmA3n2QEwDl2wWUZamnHwo5nU56TsYCqAdG5SydybRtW9d1x9cf5QFqt3F9xpqYrAZAFmKMiWs5UoC/qwY/gEYbvA6onktt24LqYxYyUnMJ/zkVzSV9u1HG2PV6RYe/BbQCCCbthYIgmBL+ymazkdvgKCTuNWb7B1dV1fTe4/6CSsuJfLKncQla2nmgaRpZTYGQJuB4PMpqCsS3n8joI3NjM9HHM5t4LoyAufl4Ab9baXwq356DeAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64 at 0x1FE812DFA00>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img=image.load_img(\"16.png\",target_size=(64,64))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd5069c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.Image.Image"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c2a7446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = image.img_to_array(img)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb6c617d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cb0f081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64, 64, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.expand_dims(x,axis=0)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5ac07fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "pred_prob = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61f342cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93688664",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name=[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\"]\n",
    "pred_id = pred_prob.argmax(axis=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b0fa964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c6cbf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the alphabet is   A\n"
     ]
    }
   ],
   "source": [
    "print(\"the alphabet is  \",str(class_name[pred_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdbbf31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
